{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start0 = time.time()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.model_selection import learning_curve, StratifiedShuffleSplit, GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "\n",
    "upsample_flag = False\n",
    "function_flag = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for this project was obtained from IBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.ibm.com/communities/analytics/watson-analytics-blog/guide-to-sample-datasets/\n",
    "url = 'http://bit.ly/gta-mlnd-capstone'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#By setting errors to 'coerce' a 'NaN will be inserted when there is an error.\n",
    "#https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html\n",
    "\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MonthlyCharges'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCharges'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['TotalCharges'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_row = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        if pd.isnull(row['TotalCharges']):\n",
    "            df.set_value(index, 'TotalCharges', row['MonthlyCharges'])\n",
    "            print(index)\n",
    "    except:\n",
    "        index_row.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['tenure'] == 1].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['ratio'] = df['TotalCharges'] / df['MonthlyCharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tenure_1 = df[df['tenure'] == 1]\n",
    "df_tenure_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['tenure'] == 2].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4]['MonthlyCharges'] * df.iloc[4]['tenure'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of customers who did not churn: {}' .format(df['Churn'].value_counts()[0]))\n",
    "print('The total number of customers who did churn: {}' .format(df['Churn'].value_counts()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.percentile(df['MonthlyCharges'], 25)\n",
    "Q3 = np.percentile(df['MonthlyCharges'], 75)\n",
    "\n",
    "step = 1.5 * (Q3 - Q1)\n",
    "    \n",
    "print('The 25% quartile: {}'.format(format(Q1)))\n",
    "print('The 75% quartile: {}'.format(format(Q3)))\n",
    "print(\"1.5 * IQR: {}\" .format(step))\n",
    "\n",
    "print(Q1 - step)\n",
    "print(Q3 + step)\n",
    "print(df['MonthlyCharges'].max())\n",
    "print(df['TotalCharges'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = np.percentile(df['TotalCharges'], 25)\n",
    "Q3 = np.percentile(df['TotalCharges'], 75)\n",
    "\n",
    "step = 1.5 * (Q3 - Q1)\n",
    "    \n",
    "print('25%: {}'.format(Q1))\n",
    "print('75%: {}'.format(Q3))\n",
    "print(\"1.5 * IQR: {}\" .format(step))\n",
    "\n",
    "print(Q1 - step)\n",
    "print(Q3 + step)\n",
    "print(df['TotalCharges'].max())\n",
    "print(df['TotalCharges'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = df[[\n",
    " 'Partner',\n",
    " 'Dependents',\n",
    " 'PhoneService',\n",
    " 'MultipleLines',\n",
    " 'InternetService',\n",
    " 'OnlineSecurity',\n",
    " 'OnlineBackup',\n",
    " 'DeviceProtection',\n",
    " 'TechSupport',\n",
    " 'StreamingTV',\n",
    " 'StreamingMovies',\n",
    " 'Contract',\n",
    " 'PaperlessBilling',\n",
    " 'PaymentMethod']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_list = df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y=\"Churn\", kind=\"count\", data=df, height=2.6, aspect=2.5, orient='h')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.title('Distribution of Customer Churn', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y='InternetService', kind='count', data=df, height=2.5, aspect=2.5, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y='PhoneService', kind='count', data=df, height=2.5, aspect=2.5, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y='PaperlessBilling', kind='count', data=df, height=2.5, aspect=2.5, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(df, y_vars=[\"tenure\"], \n",
    "                 x_vars=[\"MonthlyCharges\",\"TotalCharges\"], \n",
    "                 height=4.5, \n",
    "                 hue=\"Churn\", \n",
    "                 aspect=1.5)\n",
    "g.map(plt.scatter, alpha=0.15)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"PaperlessBilling\", height=5, aspect=.9)\n",
    "g.map(sns.barplot, \"Contract\", \"Churn\", order = ['Month-to-month', 'One year', 'Two year'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"SeniorCitizen\", height=5, aspect=.9)\n",
    "g.map(sns.barplot, \"Partner\", \"Churn\", order= ['Yes', 'No'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col='SeniorCitizen', height=5, aspect=.9)\n",
    "g.map(sns.barplot, 'PaymentMethod', 'Churn', order = ['Electronic check', \n",
    "                                                     'Mailed check',\n",
    "                                                     'Bank transfer (automatic)',\n",
    "                                                     'Credit card (automatic)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col='PhoneService', height=5, aspect=1.0)\n",
    "g.map(sns.barplot, 'PaymentMethod', 'Churn', order = ['Electronic check', \n",
    "                                                     'Mailed check',\n",
    "                                                     'Bank transfer (automatic)',\n",
    "                                                     'Credit card (automatic)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col='InternetService', height=5, aspect=.9)\n",
    "g.map(sns.barplot, 'Contract', 'Churn', order = ['Month-to-month', 'One year', 'Two year'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col='InternetService', height=5, aspect=.70)\n",
    "g.map(sns.barplot, 'Contract', 'Churn', order = ['Month-to-month', 'One year', 'Two year'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col='InternetService', height=5, aspect=.50)\n",
    "g.map(sns.barplot, 'PaperlessBilling', 'Churn', order = ['Yes', 'No'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Contract\", y=\"MonthlyCharges\", hue=\"Churn\", kind=\"box\", data=df, height=4.2, aspect=1.4)\n",
    "plt.ylabel('Total Monthly Charges ($)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Contract\", y=\"TotalCharges\", hue=\"Churn\", kind=\"box\", data=df, height=4.2, aspect=1.4)\n",
    "plt.ylabel('Total Monthly Charges ($)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Contract\", y=\"tenure\", hue=\"Churn\", kind=\"box\", data=df, height=4.2, aspect=1.4)\n",
    "plt.ylabel('Tenure (months)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y=\"Churn\", \n",
    "            x=\"TotalCharges\", \n",
    "            row=\"PaymentMethod\", \n",
    "            kind=\"box\", data=df, height=1.5, aspect=4, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_index = [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20]\n",
    "sub_list = [columns_list[x] for x in sub_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ = []\n",
    "for i in sub_list:\n",
    "    unique_.append(df[i].unique())\n",
    "type(unique_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_summary = pd.DataFrame({'Column_Header' : sub_list, 'Initial_Index': sub_index, 'Unique_Fields' : unique_})   \n",
    "df_col_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_summary.iloc[15]['Unique_Fields'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['gender'] = df['gender'].map({'Male': 0, 'Female': 1})\n",
    "df['Partner'] = df['Partner'].map({'No': 0, 'Yes': 1})\n",
    "df['Dependents'] = df['Dependents'].map({'No': 0, 'Yes': 1})\n",
    "df['PhoneService'] = df['PhoneService'].map({'No': 0, 'Yes': 1})\n",
    "df['MultipleLines'] = df['MultipleLines'].map({'No phone service': 0, 'No': 1, 'Yes': 2})\n",
    "df['InternetService'] = df['InternetService'].map({'DSL': 0, 'Fiber optic': 1, 'No': 2})\n",
    "df['OnlineSecurity'] = df['OnlineSecurity'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "df['OnlineBackup'] = df['OnlineBackup'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "df['DeviceProtection'] = df['DeviceProtection'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "df['TechSupport'] = df['TechSupport'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "df['StreamingTV'] = df['StreamingTV'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "df['StreamingMovies'] = df['StreamingMovies'].map({'No internet service': 0, 'No': 1, 'Yes': 2})\n",
    "df['Contract'] = df['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})\n",
    "df['PaperlessBilling'] = df['PaperlessBilling'].map({'No': 0, 'Yes': 1})\n",
    "df['PaymentMethod'] = df['PaymentMethod'].map({'Electronic check': 0, 'Mailed check': 1, 'Bank transfer (automatic)': 2, 'Credit card (automatic)': 3})\n",
    "# df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vect = df.drop(['customerID', 'ratio', 'MonthlyCharges'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vect_major = df_vect[df_vect['Churn'] == 0]\n",
    "df_vect_minor = df_vect[df_vect['Churn'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Major Class Shape: {}\" .format(df_vect_major.shape))\n",
    "print(\"Minor Class Shape: {}\" .format(df_vect_minor.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_minor_upsample = resample(df_vect_minor, replace=True, n_samples =5174, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minor_upsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_upsampled = pd.concat([df_vect_major, df_minor_upsample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled[df_upsampled['Churn'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled[df_upsampled['Churn'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if upsample_flag:\n",
    "    X = df_upsampled.drop('Churn', axis=1)\n",
    "    y = df_upsampled['Churn']\n",
    "else:\n",
    "    X = df_vect.drop('Churn', axis=1)\n",
    "    y = df_vect['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learning_curves(model, X_training_data, y_training_data, model_name, num_k):\n",
    "    '''\n",
    "    This function creates testing and cross validation learning curves that can be used\n",
    "    to assess the performance of a given model.\n",
    "    \n",
    "    Inputs:\n",
    "    model: This is the model object being used. \n",
    "    X_training_data: This is the training data that is used for the model training.\n",
    "    y_training_data: This is the actual response values for the training set.\n",
    "    model_name: This is the name of the model, as a text string.\n",
    "    num_k: This is the number of folks to use during the cross validation phase.\n",
    "    '''\n",
    "    plt.figure()\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X_training_data, y_training_data, cv=num_k)\n",
    "\n",
    "    plt.title(\"Learning Curves: \" + model_name + \" With Standard Scaler\")\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, \n",
    "                     train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, \n",
    "                     alpha=0.1,\n",
    "                     color=\"r\")\n",
    "\n",
    "    plt.fill_between(train_sizes, \n",
    "                     test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, \n",
    "                     alpha=0.1, color=\"b\")\n",
    "\n",
    "    plt.plot(train_sizes, \n",
    "             train_scores_mean, \n",
    "             'o-', \n",
    "             color=\"r\", \n",
    "             label=\"Training score\")\n",
    "\n",
    "    plt.plot(train_sizes, \n",
    "             test_scores_mean, \n",
    "             'o-', color=\"b\", \n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.ylim([0.0, 1.25])\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_roc_curves(model, X_test_data_, y_test_data_, model_name):\n",
    "    \n",
    "    roc_auc  = roc_auc_score(y_test_data_, model.predict(X_test_data_))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test_data_)[:,1])\n",
    "    \n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(fpr, tpr, label= model_name + ' (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: ' + model_name)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_coef_curves(model, X_data):\n",
    "    per_var = np.round(model.coef_[0], decimals = 1)\n",
    "#     labels = X_data.columns.tolist()\n",
    "    plt.bar(x = range(1, len(per_var)+1), height=per_var, tick_label = X_data.columns.tolist())\n",
    "    plt.xlabel('Feature', fontsize = 14)\n",
    "    plt.ylabel('LR Coefficient Value', fontsize = 14)\n",
    "    plt.title('LR Coefficient Values', fontsize = 14)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_results_(good_data, pca):\n",
    "    '''\n",
    "    Create a DataFrame of the PCA results\n",
    "    Includes dimension feature weights and explained variance\n",
    "    Visualizes the PCA results\n",
    "    '''\n",
    "\n",
    "    # Dimension indexing\n",
    "    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "\n",
    "    # PCA components\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns = list(good_data.keys()))\n",
    "    components.index = dimensions\n",
    "    \n",
    "    # PCA explained variance\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "\n",
    "    # Create a bar plot visualization\n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "    # Plot the feature weights as a function of the components\n",
    "    components.plot(ax = ax, kind = 'bar');\n",
    "    ax.set_ylabel(\"Feature Weights\")\n",
    "    ax.set_xticklabels(dimensions, rotation=0)\n",
    "\n",
    "    # Display the explained variance ratios\n",
    "    for i, ev in enumerate(pca.explained_variance_ratio_):\n",
    "        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n",
    "\n",
    "    # Return a concatenated DataFrame\n",
    "    return pd.concat([variance_ratios, components], axis = 1)\n",
    "\n",
    "def biplot(good_data, reduced_data, pca):\n",
    "    '''\n",
    "    Produce a biplot that shows a scatterplot of the reduced\n",
    "    data and the projections of the original features.\n",
    "    \n",
    "    good_data: original data, before transformation.\n",
    "               Needs to be a pandas dataframe with valid column names\n",
    "    reduced_data: the reduced data (the first two dimensions are plotted)\n",
    "    pca: pca object that contains the components_ attribute\n",
    "    return: a matplotlib AxesSubplot object (for any additional customization)\n",
    "    \n",
    "    This procedure is inspired by the script:\n",
    "    https://github.com/teddyroland/python-biplot\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "    # scatterplot of the reduced data    \n",
    "    ax.scatter(x=reduced_data.loc[:, 'Dimension_1'], y=reduced_data.loc[:, 'Dimension_2'], \n",
    "        facecolors='b', edgecolors='b', s=70, alpha=0.005)\n",
    "    \n",
    "    feature_vectors = pca.components_.T\n",
    "\n",
    "    # we use scaling factors to make the arrows easier to see\n",
    "    arrow_size, text_pos = 7.0, 8.0,\n",
    "\n",
    "    # projections of the original features\n",
    "    for i, v in enumerate(feature_vectors):\n",
    "        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n",
    "                  head_width=0.2, head_length=0.2, linewidth=2, color='red')\n",
    "        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color='black', \n",
    "                 ha='center', va='center', fontsize=18)\n",
    "\n",
    "    ax.set_xlabel(\"Dimension 1\", fontsize=14)\n",
    "    ax.set_ylabel(\"Dimension 2\", fontsize=14)\n",
    "    ax.set_title(\"Principal Component Plane With Original Feature Projections.\", fontsize=16);\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling Using `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standard_scaler.fit(X))\n",
    "X_std = standard_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://chrisalbon.com/machine_learning/model_evaluation/split_data_into_training_and_test_sets/\n",
    "#Tl;dr: Split the data set using train_test_split first then apply standard scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=y)\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "#This means that we don't need to run Stratified_Shuffle_Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standard_scaler.fit(X_train))\n",
    "X_train_std_df = standard_scaler.transform(X_train)\n",
    "X_test_std_df = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape of X_test: {}\".format(X_test.shape))\n",
    "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
    "print(\"Shape of y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Model development and tuning using `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR base with standard scaler\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_std_df, y_train)\n",
    "y_pred_class = lr.predict(X_test_std_df)\n",
    "\n",
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('LR coef: {}' .format(lr.coef_[0]))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef_curves(lr, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_curves(lr, X_test_std_df, y_test, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves(lr, X_train_std_df, y_train, 'Logistic Regression', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/0.15/modules/generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score\n",
    "lr_std_scores = cross_val_score(lr, X_std, y, cv=10, scoring='roc_auc')\n",
    "lr_std_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if function_flag:\n",
    "    lr_grid = LogisticRegression(random_state = 42)\n",
    "    penalty = ['l1', 'l2']\n",
    "    C = np.logspace(0, 1, 10, 100)\n",
    "    hyperparameters = dict(C=C, penalty=penalty)\n",
    "    grid_results = GridSearchCV(lr_grid, \n",
    "                                hyperparameters, \n",
    "                                verbose=3, \n",
    "                                cv=10, \n",
    "                                scoring='roc_auc').fit(X_train_std_df, y_train) #Will output the probability.\n",
    "else: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(grid_results.best_params_)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(grid_results.best_score_) #It is assumed that this combination will yield the best performing model.\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = grid_results.predict(X_test_std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ = grid_results.best_estimator_\n",
    "print(lr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_tuned = LogisticRegression(C=3.5938136638046276, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/49061575/why-when-i-use-gridsearchcv-with-roc-auc-scoring-the-score-is-different-for-gri\n",
    "lr_tuned.fit(X_train_std_df, y_train)\n",
    "y_pred_class = lr_tuned.predict(X_test_std_df) #Outputs the class type and not the probabilities.\n",
    "\n",
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('LR coef: {}' .format(lr_tuned.coef_[0]))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_curves(lr_tuned, X_test_std_df, y_test, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves(lr_tuned, X_train_std_df, y_train, 'Logistic Regression', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef_curves(lr_tuned, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report of tuned model.\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model development and tuning using `XBGClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "#https://github.com/dmlc/xgboost/tree/master/demo/guide-python\n",
    "\n",
    "#XGB base with standard scaler\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_std_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = xgb_model.predict(X_test_std_df)\n",
    "y_pred_class = [round(each) for each in y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves(xgb_model, X_train_std_df, y_train, \"XGB Classifier\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_curves(xgb_model, X_test_std_df, y_test, 'XGB Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n",
    "if function_flag:\n",
    "    xgb_grid = XGBClassifier(random_state=42)\n",
    "    learning_rate = [0.01, 0.1, 1]\n",
    "    max_depth = [1, 3, 5, 7]\n",
    "    n_estimators = [100, 1000]\n",
    "    hyperparameters = dict(learning_rate=learning_rate, max_depth=max_depth, n_estimators=n_estimators)\n",
    "    grid_results = GridSearchCV(xgb_grid, \n",
    "                                hyperparameters, \n",
    "                                verbose=3, \n",
    "                                cv=10, \n",
    "                                scoring='roc_auc').fit(X_train_std_df, y_train) #Will output the probability.\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_results.best_score_) #It is assumed that this combination will yield the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ = grid_results.best_estimator_\n",
    "print(xgb_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_tuned = grid_results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned.fit(X_train_std_df, y_train)\n",
    "y_pred_prob = xgb_tuned.predict(X_test_std_df)\n",
    "y_pred_class = [round(each) for each in y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves(xgb_tuned, X_train_std_df, y_train, \"XGB Classifier\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_curves(xgb_tuned, X_test_std_df, y_test, 'XGB Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Model development and tuning using `SVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_model = SVC(random_state=42, probability=True)\n",
    "svc_model.fit(X_train_std_df, y_train)\n",
    "y_pred_class = svc_model.predict(X_test_std_df)\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves(svc_model, X_train_std_df, y_train, \"SVC\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_curves(svc_model, X_test_std_df, y_test, 'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_flag:\n",
    "    svc_grid = SVC(random_state=42)\n",
    "    C = [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]\n",
    "    gamma = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "    kernel = ['rbf', 'poly'] \n",
    "    hyperparameters = dict(C=C, gamma=gamma, kernel=kernel)\n",
    "    grid_results = GridSearchCV(svc_grid, \n",
    "                                hyperparameters, \n",
    "                                verbose=3, \n",
    "                                cv=10, \n",
    "                                scoring='roc_auc').fit(X_train_std_df, y_train) #Will output the probability.\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_results.best_score_) #It is assumed that this combination will yield the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_ = grid_results.best_estimator_\n",
    "print(svc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_tuned = grid_results.best_estimator_\n",
    "\n",
    "# SVC(C=50, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#                 decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
    "#                 max_iter=-1, probability=True, random_state=42, shrinking=True,\n",
    "#                 tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tuned.fit(X_train_std_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc_tuned.fit(X_train_std_df, y_train)\n",
    "y_pred_class = svc_tuned.predict(X_test_std_df) #Outputs the class type and not the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start1 = time.time()\n",
    "\n",
    "learning_curves(svc_tuned, X_train_std_df, y_train, \"SVC\", 10)\n",
    "\n",
    "end1 = time.time()\n",
    "print('Elapsed time: {0:.2f} min' .format((end1 - start1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = time.time()\n",
    "\n",
    "auc_roc_curves(svc_tuned, X_test_std_df, y_test, 'SVC')\n",
    "\n",
    "end2 = time.time()\n",
    "print('Elapsed time: {0:.2f} min' .format((end2 - start2)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 `PCA` Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10).fit(X_std)\n",
    "pca_results = pca.transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Number of Principal Components', fontsize=14)\n",
    "plt.ylabel('Explained Variance', fontsize=14)\n",
    "plt.title('PCA Scree Plot', fontsize = 14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X_std)\n",
    "pca_results = pca.transform(X_std)\n",
    "\n",
    "pca_reduced_df = pd.DataFrame(pca_results, columns=[\"Dimension_1\", \"Dimension_2\"])\n",
    "pca_reduced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_(X, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot(X, pca_reduced_df, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stats.stackexchange.com/questions/244677/how-to-decide-between-pca-and-logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef_curves(lr_tuned, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run new model with the most important features (i.e., reduced feature set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced = df[['tenure','MultipleLines', 'InternetService', 'StreamingTV', 'StreamingMovies', 'Contract','PaperlessBilling','TotalCharges']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_upsampled\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standard_scaler.fit(X_train))\n",
    "X_train_std_red = standard_scaler.transform(X_train)\n",
    "X_test_std_red = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB base with standard scaler and a reduced feature set. \n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_std_red, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = xgb_model.predict(X_test_std_red)\n",
    "y_pred_class = [round(each) for each in y_pred_prob]\n",
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves(xgb_model, X_train_std_red, y_train, \"XGB Classifier\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc_curves(xgb_model, X_test_std_red, y_test, 'XGB Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if function_flag:\n",
    "    xgb_grid = XGBClassifier(random_state=42)\n",
    "    learning_rate = [0.01, 0.1, 1]\n",
    "    max_depth = [1, 3, 5, 7]\n",
    "    n_estimators = [100, 1000]\n",
    "    hyperparameters = dict(learning_rate=learning_rate, max_depth=max_depth, n_estimators=n_estimators)\n",
    "    grid_results = GridSearchCV(xgb_grid, \n",
    "                                hyperparameters, \n",
    "                                verbose=3, \n",
    "                                cv=10, \n",
    "                                scoring='roc_auc').fit(X_train_std_df, y_train) #Will output the probability.\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_results.best_score_) #It is assumed that this combination will yield the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_ = grid_results.best_estimator_\n",
    "print(xgb_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_reduced = grid_results.best_estimator_\n",
    "\n",
    "# XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#                             colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "#                             max_depth=1, min_child_weight=1, missing=None, n_estimators=1000,\n",
    "#                             n_jobs=1, nthread=None, objective='binary:logistic',\n",
    "#                             random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "#                             seed=None, silent=True, subsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned.fit(X_train_std_df, y_train)\n",
    "y_pred_prob = xgb_tuned.predict(X_test_std_df)\n",
    "y_pred_class = [round(each) for each in y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC_AUC Score: {}'. format(roc_auc_score(y_test, y_pred_class)))\n",
    "print('---')\n",
    "print('Confusion Matrix:')\n",
    "print('{}'.format(confusion_matrix(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_boost_function(X_data, y_data):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "    standard_scaler = StandardScaler()\n",
    "    standard_scaler.fit(X_train)\n",
    "    X_train_std_red_ = standard_scaler.transform(X_train)\n",
    "    X_test_std_red_ = standard_scaler.transform(X_test)\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train_std_red_, y_train)\n",
    "    \n",
    "    y_pred_prob = xgb_model.predict(X_test_std_red_)\n",
    "    y_pred_class = [round(each) for each in y_pred_prob]\n",
    "    return roc_auc_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = ['tenure','MultipleLines', 'InternetService', 'StreamingTV', 'StreamingMovies', 'Contract','PaperlessBilling','TotalCharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if upsample_flag:\n",
    "    \n",
    "    initial_feature_list = []\n",
    "    seq_feature_list = []\n",
    "    roc_scores = []\n",
    "    for each in feature_list:\n",
    "        initial_feature_list.append(each)\n",
    "        X = df_upsampled[initial_feature_list]\n",
    "        roc_scores.append(xgb_boost_function(X, y))\n",
    "        seq_feature_list.append(list(initial_feature_list))\n",
    "        \n",
    "else:\n",
    "    initial_feature_list = []\n",
    "    seq_feature_list = []\n",
    "    roc_scores = []\n",
    "    for each in feature_list:\n",
    "        initial_feature_list.append(each)\n",
    "        X = df_vect[initial_feature_list]\n",
    "        roc_scores.append(xgb_boost_function(X, y))\n",
    "        seq_feature_list.append(list(initial_feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ = np.round(roc_scores, decimals = 3)\n",
    "plt.bar(x = range(1, len(scores_)+1), height=scores_, tick_label = ['/ '.join(i) for i in seq_feature_list])\n",
    "plt.xlabel('Feature', fontsize = 14)\n",
    "plt.ylabel('XGB AUC Score', fontsize = 14)\n",
    "plt.title('AUC Score as a Function of Feature Inclusion', fontsize = 14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.ylim([0.0, 0.8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end0 = time.time()\n",
    "print('Elapsed time: {0:.2f} min' .format((end0 - start0)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
